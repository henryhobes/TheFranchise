# **LangGraph Supervisor Framework Integration**

## **Objectives**

* Integrate the **LangGraph agent orchestration framework** into our DraftOps system to serve as the “brain” that manages AI-driven decision making. We aim to establish a **Supervisor Agent** (using LangGraph’s supervisor model) that will maintain the draft context and coordinate multiple sub-agents (strategy planner, scouts, etc.) throughout the draft[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L45-L50)[auxiliobits.com](https://www.auxiliobits.com/blog/soulsync-ai-revolutionizing-mental-health-support-in-rehab-centers-with-advanced-ai/#:~:text=centric%20experience,step%20process). By using LangGraph, we leverage its built-in capabilities for persistence, state management, and streaming outputs to ensure our AI agents can function in a coordinated, coherent way even as the draft state evolves. The Supervisor will effectively hold the conversation with the AI – remembering prior reasoning and ensuring each pick’s decision is informed by previous context (enabling draft strategy coherence across rounds). This provides a robust orchestration layer, rather than writing ad-hoc prompt scripts for each agent, aligning with the plan to use a proven framework instead of a custom implementation[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L87-L95).

* Another objective is to ensure that this integration does not impede our real-time performance. LangGraph introduces some overhead (\~100-200ms per interaction)[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L96-L100), but that’s acceptable given \~60s pick windows. We will configure the framework in an **async, non-blocking manner**, so that the draft monitoring (WebSocket events) continues smoothly in parallel with AI reasoning. In summary, the goal is to set up the **foundation for AI orchestration**: by end of this spec, we have the LangGraph environment ready (connected to GPT-5) and capable of running our multi-agent workflow within the DraftOps application[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-implementation-plan.md#L2-L5).

  ## **Scope**

* **In Scope:** Bringing in the LangGraph library and initializing it for our use case. This involves installing the `langgraph` package (and possibly its OpenAI integration module) and configuring a **LangGraph Workflow** (StateGraph) that represents our draft assistant’s logic. We will create a Supervisor agent node that acts as the top-level controller. Under this Supervisor, we will later attach our specialized agents (the “scouts”, strategy planner, etc.) – for now, we can stub them or use placeholders until those are implemented in subsequent specs. The scope includes defining how draft state information will be fed into LangGraph: likely as part of the messages or via LangGraph’s memory. We will utilize LangGraph’s **thread-scoped memory** and checkpointer features[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L90-L95) to maintain continuity: for example, the Supervisor will carry over knowledge from one pick to the next (e.g., remembering which players we drafted or the reasoning from last round) in its conversation memory. We’ll set up an **InMemorySaver** (or similar) as the checkpointer so that if the connection hiccups or the process were restarted mid-draft, we could theoretically reload the last state (though full persistence is more of a Sprint 4 concern). Streaming output capability will also be set up: we’ll enable the system to handle partial responses if needed (LangGraph can stream tokens)[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L45-L50), which could later be used to display the AI’s thinking process live.

* **Out of Scope:** Implementing the full draft decision logic inside LangGraph at this stage – the detailed behavior of each agent (strategy, scouts, GM) will be handled in later specs. Initially, we might have the Supervisor simply echo something or do a trivial action to verify the pipeline. The idea is to get the framework wired in, not to finalize prompts yet. Also out of scope is any use of alternative frameworks (we won’t simultaneously integrate OpenAI’s “Swarm” or others in Sprint 2 – we stick to LangGraph as planned[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L87-L95), while keeping modularity to experiment later if needed). We are not building a new agent framework from scratch – just using LangGraph’s primitives, so writing custom lower-level concurrency frameworks is out of scope. Additionally, we won’t at this time optimize LangGraph’s performance beyond default tuning – e.g., multi-threading within LangGraph or customizing its scheduling is unnecessary until we see a need.

  ## **Deliverables**

* **LangGraph Dependency & Config:** The project’s environment will include LangGraph (and the LangChain OpenAI integration if required to use GPT-5 through LangGraph). We’ll update requirements and ensure API keys (for OpenAI GPT-5) are configured. A deliverable here is a short setup script or code snippet in our application that initializes a LangGraph `Application` or workflow for the draft assistant. For example, we might create a `DraftOpsSupervisor` object by composing LangGraph nodes: one Supervisor node controlling sub-nodes (to be filled in later). This setup will specify our model (GPT-5 via `ChatOpenAI` interface) and any global tools or memory. We will document or log that the LangGraph is successfully connecting to the model (perhaps a test call that returns a simple response to confirm the pipeline works).

* **Supervisor Agent Node:** Implementation of a placeholder **Supervisor agent** using LangGraph’s supervisor framework[auxiliobits.com](https://www.auxiliobits.com/blog/soulsync-ai-revolutionizing-mental-health-support-in-rehab-centers-with-advanced-ai/#:~:text=centric%20experience,step%20process). In practice, this could be done by defining a high-level agent that can route between different prompts/behaviors. Initially, we might implement it such that it can accept a message like “(Draft state update)” or a query and respond with a dummy recommendation, just to ensure the end-to-end flow. The supervisor is what will later orchestrate calls to our strategy and scout agents, but as a deliverable in this spec, we have the Supervisor set up as a controllable entity. For instance, we might configure it with a prompt like: “You are the DraftOps Supervisor, responsible for coordinating draft strategy.” and give it access to tools or sub-agents (to be added in future deliverables). The key part is that the **Supervisor can maintain context** between messages. We’ll verify that if we send two related prompts, the Supervisor agent remembers the first when answering the second (demonstrating statefulness via LangGraph’s memory checkpointing)[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L90-L95).

* **Memory & State Management via LangGraph:** We will deliver a mechanism for injecting our `DraftState` information into the LangGraph context at each relevant turn. This might be done by constructing a system/user message that summarizes the current state (using the utilities from the data spec) and feeding it to the Supervisor or sub-agents. Alternatively, we may leverage LangGraph’s ability to store data in memory: e.g., keep a running summary of our team needs or last picks in the agent’s long-term memory. As a concrete deliverable, we might implement a function `update_agent_context(draft_state)` that updates the Supervisor’s memory (or the content of the next prompt) with the latest pick information (who was drafted, how our roster changed, etc.). This ensures the **AI always works off the latest draft state** and that the LangGraph’s internal state mirrors the DraftState from our monitoring system[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L45-L50). We will test this by simulating a state change and confirming the next AI output reflects the new state (for example, if a player is removed from available list, the AI should no longer mention them).

* **Integration Test (LangGraph Round-Trip):** A small end-to-end test demonstrating LangGraph in action within our code. For instance, we might craft a scenario: *“It’s round 1, our pick is up, these players are available…”* and feed it to the Supervisor agent, then ensure we get a coherent response (even if placeholder). This will likely involve calling our LangGraph workflow from the `DraftMonitorConsole`. Deliverable is a console log or debug output confirming that the AI (via LangGraph) received our input and produced an output without error. Essentially, **proving that GPT-5 can be invoked through LangGraph in our system**. We expect to see the latency on the order of a couple seconds per call (e.g., a test message might return in \~2s as typical for GPT-5[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L64-L71)), which we’ll note as acceptable.

* **Documentation & Configurability:** We will document how the LangGraph integration is set up, including any parameters (like model names, temperature settings, etc.). Perhaps we’ll set GPT-5’s parameters (like using `gpt-5-standard` by default, or any routing options if available). We might expose some of these via config so we can tweak them (for example, if we want to test with GPT-4 or a cheaper model for development, we could swap it easily). The deliverable is both inline documentation in code and an update to the README or design docs describing the role of the LangGraph Supervisor in our architecture (solidifying the vision described in the overview that LangGraph is the orchestration layer for our AI-driven system[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L87-L95)).

  ## **Assumptions & Constraints**

* We assume **GPT-5 API access** is available and stable for our use. GPT-5’s multi-capability (nano/mini/standard routing) will be utilized automatically by the OpenAI integration – this means some agent calls might use smaller models for speed[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L64-L72). This is beneficial for our use case (e.g., quick scout analyses might trigger the nano model internally, keeping latency low). We’ll rely on OpenAI’s routing rather than manually selecting models, per project decision to let GPT-5 handle complexity vs. speed trade-offs[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L66-L74).

* LangGraph is a relatively new framework (with a large number of stars, indicating active use) and we assume its **documentation and stability** are sufficient. We’ll be working in Python (as is LangGraph) and expect no major compatibility issues with our asyncio-based draft monitor. One constraint is that LangGraph might encapsulate its own event loop or asynchronous calls. We need to ensure that calling LangGraph (which will in turn call the OpenAI API) is done in a non-blocking way in our asyncio application. We’ll likely use `await` on LangGraph’s `app.invoke()` or similar, which should integrate into our asyncio flow. It’s important that the WebSocket listening tasks are not stalled while the AI is thinking. We assume this can be managed either by awaiting in a separate task or using callback systems (our console app already has a callback architecture for events[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/sprint-results/sprint-1-completion-summary.md#L299-L301)).

* **Persistence:** While LangGraph offers checkpointing, we are not fully leveraging persistent storage in Sprint 2 (we won’t, for example, write conversation state to disk between runs). We assume the draft session runs in one go. However, if the WebSocket disconnects and reconnects, our Supervisor agent should still have memory intact since our process is still running (the Supervisor would not lose state on a transient connection drop). If the entire app crashed mid-draft, we are not attempting to reload the AI’s conversation from a saved checkpoint in Sprint 2 (that would be a later enhancement). So our use of checkpointer may simply be the in-memory variety to maintain state during runtime (which covers brief interruptions but not a full restart).

* We must be mindful of **overhead**: each AI call will incur \~100-200ms extra from LangGraph routing as noted[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L96-L100). This is acceptable given our time constraints, but we shouldn’t add much more on top. We will thus avoid any unnecessary layers or calls. For example, we won’t route every single WebSocket frame through LangGraph – only decision-relevant events trigger AI calls. LangGraph will not be in the hot path for every pick update, only when we need recommendations. This constraint ensures we preserve the real-time performance where needed (the state updates remain ultra-fast \<200ms outside of AI, per Sprint 1 results[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/sprint-results/sprint-1-completion-summary.md#L294-L301)).

* **Security and Usage:** We will follow OpenAI usage policies when designing prompts (no disallowed content, etc.) – likely not an issue in fantasy football domain. Also, since ESPN ToS is a concern for automation, we note that using GPT-5 via API does not violate anything (all data stays in our system and OpenAI’s). We will keep any personal data minimal (just player names, which are public figures in a sports context). This aligns with legal constraints: we continue read-only draft monitoring and AI suggestion, without automating actual site interactions[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L101-L108).

  ## **Milestones**

1. **Environment Setup:** Install and import LangGraph and related packages (e.g., `langgraph`, `langchain-openai` connectors). Verify that we can instantiate a simple LangGraph agent. *Milestone check:* run a tiny example outside the draft context (for instance, a trivial Q\&A agent) to ensure the library works with our API key. This might be using a LangGraph quickstart example – confirming that `ChatOpenAI(model="gpt-5-standard")` and a basic agent returns an answer.

2. **Define Supervisor Agent in Code:** Create a class or function that builds our `draft_assistant_workflow`. For now, define the Supervisor with a basic prompt like “You are an AI draft assistant supervisor that will coordinate strategy.” (This will be refined later, but gives it an identity). *Milestone check:* Use the LangGraph workflow to get a response to a sample input. E.g., call something like `app.invoke({"messages": [{"role": "user", "content": "Hello, Supervisor"}]})` and expect a sensible greeting back from the AI. This tests that our Supervisor pipeline from prompt to GPT-5 to response is functioning.

3. **Integrate DraftState Context Feeding:** Implement how we will pass draft context into the agent. One approach: always prepend a system message to the conversation with a summary of the current state (e.g., “System: Current Draft State → Round 5, Our Team: roster needs X, Top available players: A, B, C…”). Another approach: use LangGraph’s memory store to keep persistent facts (like our team composition). We will try a simple version first: constructing a message each time. *Milestone check:* simulate a state (say mid-draft) and run the agent with that state embedded, see if the response acknowledges the context. For example, if we include “you have no QBs yet”, we expect the AI (even in a placeholder mode) to possibly mention that if asked a relevant question. This indicates context injection is working.

4. **Async Invocation Test within DraftMonitor:** Modify the DraftMonitor/Console code to call the LangGraph supervisor at a specific event (perhaps manually trigger it for a test). For instance, when the draft starts (or via a debug command), call the Supervisor agent and print a dummy response to the console. *Milestone check:* Confirm that doing so does not freeze the WebSocket listener – i.e., the rest of the program remains responsive. We can simulate a quick draft pick during the AI call to ensure both can happen (this might require artificial delays). This tests the non-blocking integration (we may use `asyncio.create_task` to fire the AI call so that it runs concurrently). Success criteria: the AI response appears in console, and meanwhile other draft events (simulated) were still processed.

5. **Ready for Multi-Agent Expansion:** With the Supervisor and LangGraph in place, the final milestone is to ensure we have a structure ready to attach sub-agents (for strategy and scouts). We might define placeholders for those agents and attach them to the Supervisor in the workflow (perhaps using LangGraph’s multi-agent capabilities or tools that allow the Supervisor to delegate)[github.com](https://github.com/langchain-ai/langgraph-swarm-py#:~:text=LangGraph,conversation%20resumes%20with%20that%20agent)[github.com](https://github.com/langchain-ai/langgraph-swarm-py#:~:text=%2A%20Multi,tools%20for%20communication%20between%20agents). For now, these could be stub functions or simple echo agents. *Milestone check:* ensure the Supervisor can call a sub-agent stub (for example, have a dummy “Scout” tool that when invoked returns a fixed string). This will validate the mechanism of coordination, so that in the next specs we can replace stubs with actual logic. At this point, the LangGraph integration will be considered successful: the framework will be confirmed as capable of orchestrating our AI agents according to the design (fulfilling the plan’s call for a supervisor orchestration layer[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-implementation-plan.md#L2-L5)[GitHub](https://github.com/henryhobes/TheFranchise/blob/f9cf706b4a30314b8a26ca7a9c8fce5a76f31352/draftOps/docs/draftops-overview.md#L45-L50)).
